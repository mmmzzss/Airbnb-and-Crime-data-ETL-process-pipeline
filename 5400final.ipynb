{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install uszipcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access dataset with API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 13:45:07 WARN Utils: Your hostname, ElsadeMacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.206.54.163 instead (on interface en0)\n",
      "23/04/18 13:45:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/18 13:45:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#sc.stop()\n",
    "sc = SparkContext() \n",
    "config = sc.getConf()\n",
    "config.set('spark.cores.max','4')\n",
    "config.set('spark.executor.memory', '8G')\n",
    "config.set('spark.driver.maxResultSize', '8g')\n",
    "config.set('spark.kryoserializer.buffer.max', '512m')\n",
    "config.set(\"spark.driver.cores\", \"4\")\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "sc = SparkContext(conf = config) \n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for convert lat and lon to zipcode\n",
    "\n",
    "from uszipcode import SearchEngine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# define a function to get the zip code from a pair of coordinates\n",
    "search = SearchEngine()\n",
    "def get_zipcode(lat, long):\n",
    "    result = search.by_coordinates(lat, long, radius=1, returns=1)\n",
    "    if result:\n",
    "        return result[0].zipcode\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#use nympy to faster the process\n",
    "get_zipcode_v = np.vectorize(lambda lat, long: get_zipcode(lat, long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 13:45:29 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Pandas to PySpark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 13:45:39 WARN TaskSetManager: Stage 0 contains a task of very large size (1625 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/18 13:45:48 WARN TaskSetManager: Stage 1 contains a task of very large size (1625 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+\n",
      "|                name|latitude|longitude|\n",
      "+--------------------+--------+---------+\n",
      "|Skylit Midtown Ca...|40.75356|-73.98559|\n",
      "|     BlissArtsSpace!|40.68535|-73.95512|\n",
      "|Cozy Clean Guest ...| 40.8038|-73.96751|\n",
      "|Large Furnished R...|40.76457|-73.98317|\n",
      "|Large Sunny Brook...|40.66265|-73.99454|\n",
      "|Comfortable, Sunn...|40.68292|-73.96381|\n",
      "|Rooftop Deck/City...|40.76076|-73.96156|\n",
      "|Lovely, Cozy, Roo...|40.66801|-73.98784|\n",
      "|Most Central Loca...| 40.7672|-73.98464|\n",
      "|Only 2 stops to M...|40.70935|-73.95342|\n",
      "|Uptown Sanctuary ...|40.80107|-73.94255|\n",
      "|Central Park 1BR ...|40.79544|-73.94836|\n",
      "|Luminous Beautifu...|40.73405|-74.00281|\n",
      "|Cozy Garden Apart...|40.71516|-73.96448|\n",
      "|Sanctuary in East...| 40.6327|-73.93184|\n",
      "|Room in the heart...|40.76336|-73.92099|\n",
      "|UES Beautiful Blu...|40.78778|-73.94759|\n",
      "|Room with En Suit...|40.68818|-73.96383|\n",
      "|Huge Private  Flo...|40.68513|-73.96648|\n",
      "|Amazing location!...|40.71248|-73.95881|\n",
      "+--------------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'http://data.insideairbnb.com/united-states/ny/new-york-city/2023-03-06/visualisations/listings.csv'\n",
    "\n",
    "nyc_air = requests.get(url)\n",
    "\n",
    "filename = 'nyc_listings.csv'\n",
    "\n",
    "with open(filename, 'wb') as f:\n",
    "  f.write(nyc_air.content)\n",
    "\n",
    "nycbnb = pd.read_csv(filename)\n",
    "nycbnb.head()\n",
    "\n",
    "\n",
    "# Convert the Pandas DataFrame to a PySpark DataFrame\n",
    "rdd = spark.sparkContext.parallelize(nycbnb.to_dict('records'))\n",
    "dfnyc = spark.createDataFrame(rdd)\n",
    "\n",
    "dfnyc = dfnyc.select('name', 'latitude', 'longitude')\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "dfnyc.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Austin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+\n",
      "|                name|latitude|longitude|\n",
      "+--------------------+--------+---------+\n",
      "|Walk to 6th, Rain...|30.26057|-97.73441|\n",
      "|      NW Austin Room|30.45697|-97.78422|\n",
      "|Gem of a Studio n...|30.24885|-97.73587|\n",
      "|Secluded Studio @...|30.26034|-97.76487|\n",
      "|Woodland Studio L...|30.23466|-97.73682|\n",
      "|Historic house in...|30.26098|-97.73072|\n",
      "|3br/2ba Austin WF...|30.19756|-97.78754|\n",
      "|Garage Apartment ...|30.23614|-97.73225|\n",
      "|Clarksville Large...|30.28074|-97.75381|\n",
      "|SXSW pad to crash...|30.25123|-97.70463|\n",
      "|Lovely Central Au...|30.33771|-97.73708|\n",
      "|OUTDOOR LIVING IN...|30.35123|-97.76207|\n",
      "|Cozy Room in Nort...|30.37783|-97.70749|\n",
      "|    Scooby Doo House|30.28588|-97.75145|\n",
      "|Zilker's Peaceful...|30.25756|-97.76995|\n",
      "|Colorful and Quir...|30.26146|-97.77276|\n",
      "|Live Beneath Live...| 30.2309|-97.76619|\n",
      "|South 1st Garage ...|30.23644|-97.76611|\n",
      "|1 Bedroom Apt; Ab...|30.24648|-97.74913|\n",
      "|Private, Detached...|  30.313|-97.75066|\n",
      "+--------------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'http://data.insideairbnb.com/united-states/tx/austin/2023-03-16/visualisations/listings.csv'\n",
    "austin_air = requests.get(url)\n",
    "\n",
    "filename = 'austin_listings.csv'\n",
    "\n",
    "with open(filename, 'wb') as f:\n",
    "  f.write(austin_air.content)\n",
    "\n",
    "austinbnb = pd.read_csv(filename)\n",
    "austinbnb.head()\n",
    "\n",
    "# Convert the Pandas DataFrame to a PySpark DataFrame\n",
    "rdd_aus = spark.sparkContext.parallelize(austinbnb.to_dict('records'))\n",
    "dfaus = spark.createDataFrame(rdd_aus)\n",
    "\n",
    "dfaus = dfaus.select('name', 'latitude', 'longitude')\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "dfaus.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+\n",
      "|                name|latitude|longitude|\n",
      "+--------------------+--------+---------+\n",
      "|Hyde Park - Walk ...| 41.7879| -87.5878|\n",
      "|Comfy Garden Suit...| 41.9796|-87.66512|\n",
      "|Trendy Roscoe Vil...|41.94342|-87.68121|\n",
      "|Tiny Studio Apart...|41.90166|-87.68021|\n",
      "|Best in Chicago, ...|41.92918|-87.70219|\n",
      "|Historic Pullman ...|41.68843|-87.60712|\n",
      "|Pullman Art Studi...|41.68954|-87.60786|\n",
      "|3 Bedroom Across ...|41.94842|-87.65307|\n",
      "|The Biddle House ...|41.91196|-87.63981|\n",
      "|4 Bedroom Across ...|41.94774|-87.65421|\n",
      "|Pullman School Ho...|41.68912|-87.60725|\n",
      "|Lincoln Park Gues...|41.92357|-87.64947|\n",
      "|Lovely Bedroom 3 ...|41.95758| -87.7271|\n",
      "|Close to Michigan...|41.85923|-87.62198|\n",
      "|Studio Apartment ...| 42.0076| -87.6825|\n",
      "|4 Bedroom Across ...| 41.9475| -87.6542|\n",
      "|LARGE Private 1BR...|41.77738|-87.60041|\n",
      "| Musician's Quarters|41.79989|-87.59432|\n",
      "|Steps From the Be...|41.88413|-87.64315|\n",
      "|Elegant Lakefront...|41.93264|-87.64311|\n",
      "+--------------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'http://data.insideairbnb.com/united-states/il/chicago/2023-03-19/visualisations/listings.csv'\n",
    "\n",
    "chic_air = requests.get(url)\n",
    "\n",
    "filename = 'Chicago_listings.csv'\n",
    "\n",
    "with open(filename, 'wb') as f:\n",
    "  f.write(chic_air.content)\n",
    "\n",
    "chicbnb = pd.read_csv(filename)\n",
    "chicbnb.head()\n",
    "\n",
    "rdd_chi = spark.sparkContext.parallelize(chicbnb.to_dict('records'))\n",
    "dfchi = spark.createDataFrame(rdd_chi)\n",
    "\n",
    "dfchi = dfchi.select('name', 'latitude', 'longitude')\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "dfchi.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los Angeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 13:47:23 WARN TaskSetManager: Stage 6 contains a task of very large size (1663 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+\n",
      "|                name|  latitude|   longitude|\n",
      "+--------------------+----------+------------+\n",
      "|Phillips Ranch ro...|34.0318556|-117.7779937|\n",
      "|Family oriented home|  34.43925|  -118.44409|\n",
      "|        方便易居驿站|  34.04848|  -117.76887|\n",
      "|TERRANEA OCEANFRN...|  33.73842|   -118.3958|\n",
      "|Brand New Duplex ...|  33.74842|    -118.311|\n",
      "| Zuma Malibu Retreat|  34.03979|  -118.86885|\n",
      "|Single BR with Ki...|  33.73928|  -118.29911|\n",
      "|Premium & New rem...|  33.99764|  -117.78979|\n",
      "|Relaxing and priv...|  33.90055|  -118.26036|\n",
      "|Unique Starry Nig...|  34.68979|  -117.77503|\n",
      "|Awesome new and i...|  33.83587|  -118.28795|\n",
      "|( VAL VERDE ) Spa...|  34.44707|  -118.67733|\n",
      "|Million Dollar Vi...|  33.78223|  -118.31712|\n",
      "|Amy’s FrontHouse ...|    33.984|  -118.05205|\n",
      "|A Perfect Brand N...|  33.89942|  -118.04855|\n",
      "|3 Bedrooms 4 beds...|  33.79169|  -118.34688|\n",
      "|1. Private Rm, La...| 34.681137|  -118.08123|\n",
      "|#7Artesia_ Disney...|  33.86314|   -118.0753|\n",
      "|        Master Suite|  34.68917|  -118.07447|\n",
      "|Modern LA vacatio...|  33.90946|  -118.12683|\n",
      "+--------------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 13:47:23 WARN TaskSetManager: Stage 7 contains a task of very large size (1663 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "url = 'http://data.insideairbnb.com/united-states/ca/los-angeles/2023-03-07/visualisations/listings.csv'\n",
    "\n",
    "LA_air = requests.get(url)\n",
    "\n",
    "filename = 'LA_listings.csv'\n",
    "\n",
    "with open(filename, 'wb') as f:\n",
    "  f.write(LA_air.content)\n",
    "\n",
    "LAbnb = pd.read_csv(filename)\n",
    "\n",
    "rdd_la = spark.sparkContext.parallelize(LAbnb.to_dict('records'))\n",
    "dfla = spark.createDataFrame(rdd_la)\n",
    "\n",
    "dfla = dfla.select('name', 'latitude', 'longitude')\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "dfla.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## San Francisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+\n",
      "|                name|latitude| longitude|\n",
      "+--------------------+--------+----------+\n",
      "|Bright, Modern Ga...|37.77028|-122.43317|\n",
      "|  Creative Sanctuary|37.74474|-122.42089|\n",
      "|Friendly Room Apt...|37.76555|-122.45213|\n",
      "|Historic Alamo Sq...|37.77564|-122.43642|\n",
      "|Mission Sunshine,...| 37.7603|-122.42197|\n",
      "|Elegant & Cozy w/...| 37.7175|-122.39698|\n",
      "|Stylish, Spacious...| 37.7844|-122.47932|\n",
      "|Mission's \"La Len...|37.74766|-122.42041|\n",
      "|Classic Nob Hill ...|37.79249|-122.41499|\n",
      "|Sunny/Sunset view...|37.76901|-122.44701|\n",
      "|Central San Franc...|37.77196|-122.43477|\n",
      "|Sunny/Sunset view...|37.76932|-122.44723|\n",
      "|Perfectly located...|37.76237|-122.42992|\n",
      "|Lake Street Singl...|37.78546|-122.46148|\n",
      "|Loft-like Apt./Ga...|37.74916|-122.42971|\n",
      "|Prime Location - ...|37.79233|-122.42431|\n",
      "|Best Views in all...|37.74633|-122.44591|\n",
      "|Spacious 1brm w/ ...|37.72833|-122.44125|\n",
      "|Lower Haight Urba...|37.77247|-122.43301|\n",
      "|Comfortable space...|37.77988|-122.48488|\n",
      "+--------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'http://data.insideairbnb.com/united-states/ca/san-francisco/2023-03-06/visualisations/listings.csv'\n",
    "\n",
    "sf_air = requests.get(url)\n",
    "\n",
    "filename = 'sf_listings.csv'\n",
    "\n",
    "with open(filename, 'wb') as f:\n",
    "  f.write(sf_air.content)\n",
    "\n",
    "sfbnb = pd.read_csv(filename)\n",
    "\n",
    "rdd_sf = spark.sparkContext.parallelize(sfbnb.to_dict('records'))\n",
    "dfsf = spark.createDataFrame(rdd_sf)\n",
    "\n",
    "dfsf = dfsf.select('name', 'latitude', 'longitude')\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "dfsf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 13:47:34 WARN TaskSetManager: Stage 10 contains a task of very large size (1625 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "114433"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb = dfnyc.union(dfaus).union(dfchi).union(dfla).union(dfsf)\n",
    "airbnb.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the zip code for Airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 13:48:52 WARN TaskSetManager: Stage 13 contains a task of very large size (1625 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "114433"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_df = airbnb.toPandas()\n",
    "airbnb_zip = get_zipcode_v(airbnb_df['latitude'], airbnb_df['longitude'])\n",
    "\n",
    "airbnb_df['zipcode'] = airbnb_zip\n",
    "airbnb_df = airbnb_df[['name', 'zipcode']]\n",
    "len(airbnb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_df.to_csv('airbnb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "response_ny = requests.get('https://data.cityofnewyork.us/resource/uip8-fykc.csv?$limit=189774')\n",
    "\n",
    "nycrime = 'ny_crime.csv'\n",
    "\n",
    "with open(nycrime, 'wb') as f:\n",
    "  f.write(response_ny.content)\n",
    "\n",
    "\n",
    "nyc_crime_sdf = sqlContext.read.option(\"header\", \"true\") \\\n",
    "                         .option(\"delimiter\", \",\") \\\n",
    "                         .option(\"inferSchema\", \"true\") \\\n",
    "                         .csv(nycrime)\n",
    "\n",
    "# NYC crime data clean\n",
    "\n",
    "#time\n",
    "from pyspark.sql.functions import col, date_format\n",
    "nyc_crime_sdf = nyc_crime_sdf.withColumn(\"am_pm\", date_format(col(\"arrest_date\"), \"a\"))\n",
    "\n",
    "#keep only the crime,\"am_pm\" longitude, and latitude\n",
    "nyc_crime_sdf = nyc_crime_sdf.select(\"ofns_desc\",\"am_pm\", \"longitude\", \"latitude\").withColumnRenamed(\"ofns_desc\", \"crime\")\n",
    "\n",
    "#add city column\n",
    "from pyspark.sql.functions import lit\n",
    "nyc_crime_sdf = nyc_crime_sdf.withColumn(\"city\", lit(\"New_York\"))\n",
    "\n",
    "#drop all the null\n",
    "nyc_crime_sdf = nyc_crime_sdf.na.replace(\"(null)\", None)\n",
    "nyc_crime_sdf = nyc_crime_sdf.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Austin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "response_aus = requests.get('https://data.austintexas.gov/resource/fdj4-gpfu.csv?$limit=2371193&$where=occ_date_time%20%3E%3D%20%272022-01-01T00:00:00%27')\n",
    "\n",
    "auscrime = 'aus_crime.csv'\n",
    "\n",
    "with open(auscrime, 'wb') as f:\n",
    "  f.write(response_aus.content)\n",
    "\n",
    "\n",
    "auscrime_sdf = sqlContext.read.option(\"header\", \"true\") \\\n",
    "                         .option(\"delimiter\", \",\") \\\n",
    "                         .option(\"inferSchema\", \"true\") \\\n",
    "                         .csv(auscrime)\n",
    "\n",
    "# austin crime data clean\n",
    "\n",
    "#time\n",
    "from pyspark.sql.functions import col, date_format\n",
    "auscrime_sdf = auscrime_sdf.withColumn(\"am_pm\", date_format(col(\"occ_date_time\"), \"a\"))\n",
    "\n",
    "#keep only the crime,\"am_pm\" longitude, and latitude\n",
    "auscrime_sdf = auscrime_sdf.select(\"crime_type\",\"am_pm\", \"zip_code\").withColumnRenamed(\"crime_type\", \"crime\").withColumnRenamed(\"zip_code\", \"zipcode\")\n",
    "\n",
    "#add city column\n",
    "from pyspark.sql.functions import lit\n",
    "auscrime_sdf = auscrime_sdf.withColumn(\"city\", lit(\"Austin\"))\n",
    "\n",
    "#drop all the null\n",
    "auscrime_sdf = auscrime_sdf.na.replace(\"(null)\", None)\n",
    "auscrime_sdf = auscrime_sdf.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "response_la = requests.get('https://data.lacity.org/resource/2nrs-mtv8.csv?$limit=699280&$where=date_occ%20%3E%3D%20%272022-01-01T00:00:00%27')\n",
    "\n",
    "lacrime = 'la_crime.csv'\n",
    "\n",
    "with open(lacrime, 'wb') as f:\n",
    "  f.write(response_la.content)\n",
    "\n",
    "\n",
    "lacrime_sdf = sqlContext.read.option(\"header\", \"true\") \\\n",
    "                         .option(\"delimiter\", \",\") \\\n",
    "                         .option(\"inferSchema\", \"true\") \\\n",
    "                         .csv(lacrime)\n",
    "\n",
    "# LA crime data clean\n",
    "\n",
    "#time\n",
    "from pyspark.sql.functions import col, date_format\n",
    "lacrime_sdf = lacrime_sdf.withColumn(\"am_pm\", date_format(col(\"date_occ\"), \"a\"))\n",
    "\n",
    "#keep only the crime,\"am_pm\" longitude, and latitude\n",
    "lacrime_sdf = lacrime_sdf.select(\"crm_cd_desc\",\"am_pm\", \"lon\",\"lat\").withColumnRenamed(\"crm_cd_desc\", \"crime\").withColumnRenamed(\"lat\", \"latitude\").withColumnRenamed(\"lon\", \"longitude\")\n",
    "\n",
    "#add city column\n",
    "from pyspark.sql.functions import lit\n",
    "lacrime_sdf = lacrime_sdf.withColumn(\"city\", lit(\"LA\"))\n",
    "\n",
    "#drop all the null\n",
    "lacrime_sdf = lacrime_sdf.na.replace(\"(null)\", None)\n",
    "lacrime_sdf = lacrime_sdf.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "response_chi = requests.get('https://data.cityofchicago.org/resource/x2n5-8w5q.csv?$limit=247370&$where=date_of_occurrence%20%3E%3D%20%272022-01-01T00:00:00%27')\n",
    "\n",
    "chicrime = 'chi_crime.csv'\n",
    "\n",
    "with open(chicrime, 'wb') as f:\n",
    "  f.write(response_chi.content)\n",
    "\n",
    "\n",
    "chicrime_sdf = sqlContext.read.option(\"header\", \"true\") \\\n",
    "                         .option(\"delimiter\", \",\") \\\n",
    "                         .option(\"inferSchema\", \"true\") \\\n",
    "                         .csv(chicrime)\n",
    "\n",
    "# Chicago crime data clean\n",
    "\n",
    "#time\n",
    "from pyspark.sql.functions import col, date_format\n",
    "chicrime_sdf = chicrime_sdf.withColumn(\"am_pm\", date_format(col(\"date_of_occurrence\"), \"a\"))\n",
    "\n",
    "#keep only the crime,\"am_pm\" longitude, and latitude\n",
    "chicrime_sdf = chicrime_sdf.select(\"_primary_decsription\",\"am_pm\", \"longitude\",\"latitude\").withColumnRenamed(\"_primary_decsription\", \"crime\")\n",
    "\n",
    "#add city column\n",
    "from pyspark.sql.functions import lit\n",
    "chicrime_sdf = chicrime_sdf.withColumn(\"city\", lit(\"Chicage\"))\n",
    "\n",
    "#drop all the null\n",
    "chicrime_sdf = chicrime_sdf.na.replace(\"(null)\", None)\n",
    "chicrime_sdf = chicrime_sdf.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## San Francisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "response_sf = requests.get('https://data.sfgov.org/resource/wg3w-h783.csv?$limit=717040&$where=incident_date%20%3E%3D%20%272022-01-01T00:00:00%27')\n",
    "\n",
    "sfcrime = 'sf_crime.csv'\n",
    "\n",
    "with open(sfcrime, 'wb') as f:\n",
    "  f.write(response_sf.content)\n",
    "\n",
    "\n",
    "sfcrime_sdf = sqlContext.read.option(\"header\", \"true\") \\\n",
    "                         .option(\"delimiter\", \",\") \\\n",
    "                         .option(\"inferSchema\", \"true\") \\\n",
    "                         .csv(sfcrime)\n",
    "\n",
    "# San Fransico crime data clean\n",
    "\n",
    "#time\n",
    "from pyspark.sql.functions import col, date_format\n",
    "sfcrime_sdf = sfcrime_sdf.withColumn(\"am_pm\", date_format(col(\"incident_datetime\"), \"a\"))\n",
    "\n",
    "#keep only the crime,\"am_pm\" longitude, and latitude\n",
    "sfcrime_sdf = sfcrime_sdf.select(\"incident_category\",\"am_pm\", \"longitude\",\"latitude\").withColumnRenamed(\"incident_category\", \"crime\")\n",
    "\n",
    "#add city column\n",
    "from pyspark.sql.functions import lit\n",
    "sfcrime_sdf = sfcrime_sdf.withColumn(\"city\", lit(\"SF\"))\n",
    "\n",
    "#drop all the null\n",
    "sfcrime_sdf = sfcrime_sdf.na.replace(\"(null)\", None)\n",
    "sfcrime_sdf = sfcrime_sdf.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union all the crime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_data_NOaus = nyc_crime_sdf.union(lacrime_sdf).union(chicrime_sdf).union(sfcrime_sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the zip code for Crime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "crime_data_NOaus_df = crime_data_NOaus.toPandas()\n",
    "crime_data_NOaus_zip = get_zipcode_v(crime_data_NOaus_df['latitude'], crime_data_NOaus_df['longitude'])\n",
    "\n",
    "crime_data_NOaus_df['zipcode'] = crime_data_NOaus_zip\n",
    "crime_data_NOaus_df = crime_data_NOaus_df[['crime','am_pm','city', 'zipcode']]\n",
    "crime_data_NOaus_df = crime_data_NOaus_df.dropna()\n",
    "\n",
    "\n",
    "crime_data_aus_df = auscrime_sdf.toPandas()\n",
    "crime_data = pd.concat([crime_data_NOaus_df, crime_data_aus_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_data['crime'] = crime_data['crime'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tail'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/82/wzfhn24j3k1_5t_bv7qglk4r0000gn/T/ipykernel_88286/326513383.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrime_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tail'"
     ]
    }
   ],
   "source": [
    "crime_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_data.to_csv('crime_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jB2X5XYCJqL_"
   },
   "source": [
    "# Join Airbnb and Crime dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb = pd.read_csv('airbnb.csv')\n",
    "crime = pd.read_csv('crime_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "hOtwhltuJtK9"
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost',27017) ## or MongoClient(\"localhost:27017\")\n",
    "db = client.apan5400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collection.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collection1.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('crime_data.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    # Create a reader object to read the CSV file\n",
    "    reader = csv.DictReader(csvfile)\n",
    "\n",
    "    # Create an empty list to hold the JSON objects\n",
    "    crime_data = []\n",
    "\n",
    "    # Iterate over each row in the CSV file\n",
    "    for row in reader:\n",
    "        # Add the row as a dictionary to the list\n",
    "        crime_data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7fd820b8efd0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection1 = db.crime\n",
    "collection1.insert_many(crime_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime list function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def details(zipcode):\n",
    "    crimelist = list(collection1.aggregate([\n",
    "    {\"$match\": {\"zipcode\": zipcode}},\n",
    "    {\"$group\": {\"_id\": \"$crime\",\"total_crime_events\": { \"$sum\": 1 }}},\n",
    "        { \"$sort\": { \"total_crime_events\": -1 } }])\n",
    "                    )\n",
    "    \n",
    "    return pd.DataFrame(crimelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day/nigh crime table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "\n",
    "def DayNight_info(zipcode):\n",
    "    results = list(collection1.aggregate([\n",
    "        {'$match': {'zipcode': zipcode}},\n",
    "        {'$group': {'_id': '$am_pm','count': {'$sum': 1}}}]))\n",
    "    \n",
    "    labels = [r['_id'] for r in results]\n",
    "    counts = [r['count'] for r in results]\n",
    "\n",
    "    if len(results) == 1 and labels[0] == \"AM\":\n",
    "        return (\"This city's open data has not yet updated day/night crime information\")      \n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "def DayNight_info(zipcode):\n",
    "    results = list(collection1.aggregate([\n",
    "        {'$match': {'zipcode': zipcode}},\n",
    "        {'$group': {'_id': '$am_pm','count': {'$sum': 1}}}]))\n",
    "    \n",
    "    counts = [r['count'] for r in results]\n",
    "    total = sum(counts)\n",
    "\n",
    "    if len(results) == 1 and results[0]['_id'] == \"AM\":\n",
    "        return \"This city's open data has not yet updated day/night crime information\"\n",
    "      \n",
    "    AM_pct = 100 * counts[0] / total\n",
    "    PM_pct = 100 * counts[1] / total\n",
    "\n",
    "    return f\"Around this area, The crimes that happened in the morning are {AM_pct:.2f}%, and the crimes that happened at night are {PM_pct:.2f}%.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 5 cirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top5_crime(zipcode):\n",
    "    \n",
    "    y = int(zipcode)\n",
    "    x = crime.loc[crime['zipcode'] == y, 'city'].unique()\n",
    "    city = x[0]\n",
    "    top5 = list(collection1.aggregate([\n",
    "    {\"$match\": {\"city\": city}},\n",
    "    {\"$group\": {\"_id\": \"$crime\",\"total_crimes\": { \"$sum\": 1 }}},\n",
    "    { \"$sort\": { \"total_crimes\": -1 } },\n",
    "    { \"$limit\": 5},\n",
    "    { \"$project\": {\"_id\":0, \"crime\": \"$_id\",\"number\": \"$total_crimes\"}}])\n",
    "               )\n",
    "\n",
    "    return pd.DataFrame(top5), city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crime</th>\n",
       "      <th>am_pm</th>\n",
       "      <th>city</th>\n",
       "      <th>zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>837542</th>\n",
       "      <td>family disturbance</td>\n",
       "      <td>PM</td>\n",
       "      <td>Austin</td>\n",
       "      <td>78751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837543</th>\n",
       "      <td>harassment</td>\n",
       "      <td>AM</td>\n",
       "      <td>Austin</td>\n",
       "      <td>78704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837544</th>\n",
       "      <td>theft by shoplifting</td>\n",
       "      <td>PM</td>\n",
       "      <td>Austin</td>\n",
       "      <td>78759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837545</th>\n",
       "      <td>doc offensive gesture</td>\n",
       "      <td>PM</td>\n",
       "      <td>Austin</td>\n",
       "      <td>78758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837546</th>\n",
       "      <td>burglary non residence</td>\n",
       "      <td>AM</td>\n",
       "      <td>Austin</td>\n",
       "      <td>78758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         crime am_pm    city  zipcode\n",
       "837542      family disturbance    PM  Austin    78751\n",
       "837543              harassment    AM  Austin    78704\n",
       "837544    theft by shoplifting    PM  Austin    78759\n",
       "837545   doc offensive gesture    PM  Austin    78758\n",
       "837546  burglary non residence    AM  Austin    78758"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template, Markup\n",
    "app = Flask(\"Interactive App\")\n",
    "\n",
    "@app.route('/')\n",
    "def my_form():\n",
    "    return render_template(\"5400Proj.html\")\n",
    "\n",
    "@app.route('/', methods=['POST'])\n",
    "def my_form_post():\n",
    "    val = str(request.form['zipcode'])\n",
    "    name = request.form['airbnb_name']\n",
    "\n",
    "    if name == 'none':\n",
    "        zipcode = val\n",
    "    else:\n",
    "        x = airbnb.loc[airbnb['name'] == name, 'zipcode']\n",
    "        zipcode = x.iloc[0]\n",
    "\n",
    "    \n",
    "    result = details(zipcode)\n",
    "    table_html = result.to_html(index=False)\n",
    "    \n",
    "    AM_PM_info = DayNight_info(zipcode)\n",
    "    \n",
    "    top5, city = get_top5_crime(zipcode)\n",
    "    table_html2 = top5.to_html(index=False)\n",
    "    \n",
    "    return render_template('result.html', city = city, table = Markup(table_html), AMPMinfo = AM_PM_info, top5 = Markup(table_html2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'Interactive App'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://localhost:5042\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug:127.0.0.1 - - [18/Apr/2023 15:43:18] \"GET / HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [18/Apr/2023 15:43:19] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "INFO:werkzeug:127.0.0.1 - - [18/Apr/2023 15:43:25] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run(host='localhost', port=5042)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wrpHzlMgHsjw",
    "pQzo9ZM5gUV6",
    "eMFNw1Z0StVx",
    "S9X0Y8AhSvhJ",
    "COmK8bATYgYN",
    "-eohzglZU4FS",
    "KvyDzSTSXtqo",
    "_fKbw8PVWLbX",
    "YgO2h0SpF4Hp",
    "qU7VdSOYGDT-",
    "kDAR9uONHp-8",
    "V2qhOsWLJ6PP",
    "Hkoq9AhONyw5",
    "jB2X5XYCJqL_",
    "v1mu2PcOSiPO"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
